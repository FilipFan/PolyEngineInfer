package dev.filipfan.polyengineinfer.api

import kotlinx.coroutines.flow.Flow

/**
 * A unified interface for on-device Large Language Model inference engines.
 * It abstracts the core functionalities like loading, unloading, and text generation.
 */
interface LlmInferenceEngine {
    /**
     * Asynchronously loads the model from the given file path.
     * This is a suspending function as model loading can be a long-running operation.
     *
     * @param path The path to the model files.
     * @param options The configurable parameters for the inference.
     */
    suspend fun load(path: LlmModelFiles, options: LlmInferenceOptions)

    /**
     * Asynchronously unloads the model and releases all associated resources.
     */
    suspend fun unload()

    /**
     * Generates a response stream for the given prompt.
     *
     * @param prompt The input text to the model.
     * @return A [Flow] of [String] that emits tokens as they are generated by the model.
     */
    fun generate(prompt: String): Flow<String>

    /**
     * Gets the prompt token size for the most recent content generation.
     *
     * @returns The size of the latest prompt in tokens. `-1` indicates no generation yet.
     */
    fun getLatestGenerationPromptTokenSize(): Int
}
